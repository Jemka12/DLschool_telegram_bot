{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_bot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5bf3f43a0924c91980bed968bf07937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_baa9c0bdcbef4007a095456755e7553a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_208db3b7a80942b981795c1753b3458e",
              "IPY_MODEL_329ba539bf244d4db19d2ac1f7c03bd7"
            ]
          }
        },
        "baa9c0bdcbef4007a095456755e7553a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "208db3b7a80942b981795c1753b3458e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9092c0d95ecd4bdd879ad16725c850f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8057770337904687833f9c197cfaeb9b"
          }
        },
        "329ba539bf244d4db19d2ac1f7c03bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a254352b8f64f68bb4d50474df44fa3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 548M/548M [00:02&lt;00:00, 203MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06acd07746af4821be617a3a275dda16"
          }
        },
        "9092c0d95ecd4bdd879ad16725c850f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8057770337904687833f9c197cfaeb9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a254352b8f64f68bb4d50474df44fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06acd07746af4821be617a3a275dda16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "276_J7j5NcYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca277519-655e-44c9-fe84-27b495040ebd"
      },
      "source": [
        "!pip install python-telegram-bot\n",
        "!pip install -U ipykernel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-telegram-bot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/5b/449509ac7da9392ffeadb5413cda48158caae095e523a5facdeca0613f97/python_telegram_bot-12.4.0-py2.py3-none-any.whl (357kB)\n",
            "\r\u001b[K     |█                               | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 7.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (4.4.1)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (0.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (2019.11.28)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 49.2MB/s \n",
            "\u001b[?25hCollecting tornado>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 78.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.13.2)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.19)\n",
            "Building wheels for collected packages: tornado\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.3-cp36-cp36m-linux_x86_64.whl size=423198 sha256=bc854da20e03af84296c375c9baf9489371d5e5d2e830cda99a255ae54312de8\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/bf/40/2f6ef700f48401ca40e5e3dd7d0e3c0a90e064897b7fe5fc08\n",
            "Successfully built tornado\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cryptography, tornado, python-telegram-bot\n",
            "  Found existing installation: tornado 4.5.3\n",
            "    Uninstalling tornado-4.5.3:\n",
            "      Successfully uninstalled tornado-4.5.3\n",
            "Successfully installed cryptography-2.8 python-telegram-bot-12.4.0 tornado-6.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ipykernel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/62/d1a5d654b7a21bd3eb99be1b59a608cc18a7a08ed88495457a87c40a0495/ipykernel-5.1.4-py3-none-any.whl (116kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 29.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 81kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (6.0.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.3.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (17.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (4.6.1)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel) (0.1.8)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.6.0, but you'll have ipykernel 5.1.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ipykernel\n",
            "  Found existing installation: ipykernel 4.6.1\n",
            "    Uninstalling ipykernel-4.6.1:\n",
            "      Successfully uninstalled ipykernel-4.6.1\n",
            "Successfully installed ipykernel-5.1.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpHpCSbQOxa6",
        "colab_type": "text"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUPu1DpEO1iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from scipy import misc\n",
        "import copy\n",
        "import numpy as np \n",
        "\n",
        "import os\n",
        "from telegram.ext import Updater, MessageHandler, Filters, CommandHandler, ConversationHandler, CallbackQueryHandler\n",
        "from telegram.ext.dispatcher import run_async\n",
        "from telegram import InlineKeyboardButton, InlineKeyboardMarkup\n",
        "import logging\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3FQM7wsO_L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target, ):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # we 'detach' the target content from the tree used\n",
        "        # to dynamically compute the gradient: this is a stated value,\n",
        "        # not a variable. Otherwise the forward method of the criterion\n",
        "        # will throw an error.\n",
        "        self.target = target.detach()  # это константа. Убираем ее из дерева вычеслений\n",
        "        self.loss = F.mse_loss(self.target, self.target)  # to initialize with something\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHP-xYxePB4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = self.gram_matrix(target_feature).detach()\n",
        "        self.loss = F.mse_loss(self.target, self.target)  # to initialize with something\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = self.gram_matrix(input)\n",
        "        self.loss = F.mse_loss(G, self.target)\n",
        "        return input\n",
        "\n",
        "    def gram_matrix(self,input):\n",
        "        batch_size, h, w, f_map_num = input.size()  # batch size(=1)\n",
        "        # b=number of feature maps\n",
        "        # (h,w)=dimensions of a feature map (N=h*w)\n",
        "\n",
        "        features = input.view(batch_size * h, w * f_map_num)  # resise F_XL into \\hat F_XL\n",
        "\n",
        "        G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "        # we 'normalize' the values of the gram matrix\n",
        "        # by dividing by the number of element in each feature maps.\n",
        "        return G.div(batch_size * h * w * f_map_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TavUrCkjPEVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
        "        # directly work with image Tensor of shape [B x C x H x W].\n",
        "        # B is batch size. C is number of channels. H is height and W is width.\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # normalize img\n",
        "        return (img - self.mean) / self.std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXOCsfViPHj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleTransferModel:\n",
        "    def __init__(self):\n",
        "        # Сюда необходимо перенести всю иницализацию, вроде загрузки свеерточной сети и т.д.\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "        pass\n",
        "\n",
        "\n",
        "    def transfer_style(self, content_img_stream, style_img_stream, num_steps=500,\n",
        "                        style_weight=100000000, content_weight=1):\n",
        "        content_img = self.process_image(content_img_stream)\n",
        "        style_img = self.process_image(style_img_stream)\n",
        "        input_img = content_img.clone().detach()\n",
        "        \"\"\"Run the style transfer.\"\"\"\n",
        "        print('Building the style transfer model..')\n",
        "        model, style_losses, content_losses = self.get_style_model_and_losses(style_img,\n",
        "                                                                         content_img,)\n",
        "        optimizer = self.get_input_optimizer(input_img)\n",
        "\n",
        "        print('Optimizing..')\n",
        "        run = [0]\n",
        "        while run[0] <= num_steps:\n",
        "\n",
        "            def closure():\n",
        "                # correct the values\n",
        "                # это для того, чтобы значения тензора картинки не выходили за пределы [0;1]\n",
        "                input_img.data.clamp_(0, 1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                model(input_img)\n",
        "\n",
        "                style_score = 0\n",
        "                content_score = 0\n",
        "\n",
        "                for sl in style_losses:\n",
        "                    style_score += sl.loss\n",
        "                for cl in content_losses:\n",
        "                    content_score += cl.loss\n",
        "\n",
        "                # взвешивание ощибки\n",
        "                style_score *= style_weight\n",
        "                content_score *= content_weight\n",
        "\n",
        "                loss = style_score + content_score\n",
        "                loss.backward()\n",
        "\n",
        "                run[0] += 1\n",
        "                if run[0] % 50 == 0:\n",
        "                    print(\"run {}:\".format(run))\n",
        "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                        style_score.item(), content_score.item()))\n",
        "                    print()\n",
        "\n",
        "                return style_score + content_score\n",
        "\n",
        "            optimizer.step(closure)\n",
        "\n",
        "        # a last correction...\n",
        "        input_img.data.clamp_(0, 1)\n",
        "        image = input_img.cpu().clone().detach()\n",
        "        unloader = transforms.ToPILImage()\n",
        "        return unloader(image[0])\n",
        "\n",
        "\n",
        "    def get_input_optimizer(self, input_img):\n",
        "        #добоваляет содержимое тензора катринки в список изменяемых оптимизатором параметров\n",
        "        optimizer = optim.LBFGS([input_img.requires_grad_()]) \n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def process_image(self, img_stream):\n",
        "        # TODO размер картинки, device и трансформации не меняются в течении всей работы модели,\n",
        "        # поэтому их нужно перенести в конструктор!\n",
        "        imsize = 720\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(device)\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize(imsize),  # нормируем размер изображения\n",
        "            transforms.CenterCrop(imsize),\n",
        "            transforms.ToTensor()])  # превращаем в удобный формат\n",
        "\n",
        "        image = Image.open(img_stream)\n",
        "        image = loader(image).unsqueeze(0)\n",
        "        return image.to(device, torch.float)\n",
        "\n",
        "    def get_style_model_and_losses(self, style_img, content_img):\n",
        "        content_layers = ['conv_4']\n",
        "        style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "        cnn = copy.deepcopy(self.cnn)\n",
        "\n",
        "        # normalization module\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "        # just in order to have an iterable access to or list of content/syle\n",
        "        # losses\n",
        "        content_losses = []\n",
        "        style_losses = []\n",
        "\n",
        "        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
        "        # to put in modules that are supposed to be activated sequentially\n",
        "        model = nn.Sequential(normalization)\n",
        "\n",
        "        i = 0  # increment every time we see a conv\n",
        "        for layer in cnn.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                i += 1\n",
        "                name = 'conv_{}'.format(i)\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = 'relu_{}'.format(i)\n",
        "                # Переопределим relu уровень\n",
        "                layer = nn.ReLU(inplace=False)\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = 'pool_{}'.format(i)\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = 'bn_{}'.format(i)\n",
        "            else:\n",
        "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            if name in content_layers:\n",
        "                # add content loss:\n",
        "                target = model(content_img).detach()\n",
        "                content_loss = ContentLoss(target)\n",
        "                model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "                content_losses.append(content_loss)\n",
        "\n",
        "            if name in style_layers:\n",
        "                # add style loss:\n",
        "                target_feature = model(style_img).detach()\n",
        "                style_loss = StyleLoss(target_feature)\n",
        "                model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "                style_losses.append(style_loss)\n",
        "\n",
        "        # выбрасываем все уровни после последенего styel loss или content loss\n",
        "        for i in range(len(model) - 1, -1, -1):\n",
        "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "                break\n",
        "\n",
        "        model = model[:(i + 1)]\n",
        "\n",
        "        return model, style_losses, content_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmDfT7fQl7p4",
        "colab_type": "text"
      },
      "source": [
        "# CycleGan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnp4QlLTmUA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "578875a3-e909-4b87-e02b-7255b4090192"
      },
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2194, done.\u001b[K\n",
            "remote: Total 2194 (delta 0), reused 0 (delta 0), pack-reused 2194\u001b[K\n",
            "Receiving objects: 100% (2194/2194), 8.01 MiB | 10.60 MiB/s, done.\n",
            "Resolving deltas: 100% (1423/1423), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogdal6ULmW4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/pytorch-CycleGAN-and-pix2pix/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvfsWItM0ILJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqt6a2GVmXYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnZzFPUDyHU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "09043a50-77e8-4c1c-af0a-f167ded02491"
      },
      "source": [
        "!bash ./scripts/download_cyclegan_model.sh winter2summer_yosemite"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: available models are apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower\n",
            "Specified [winter2summer_yosemite]\n",
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2020-02-08 12:31:36--  http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/winter2summer_yosemite.pth\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.189.73\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.189.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45575747 (43M)\n",
            "Saving to: ‘./checkpoints/winter2summer_yosemite_pretrained/latest_net_G.pth’\n",
            "\n",
            "./checkpoints/winte 100%[===================>]  43.46M  13.9MB/s    in 3.1s    \n",
            "\n",
            "2020-02-08 12:31:39 (13.9 MB/s) - ‘./checkpoints/winter2summer_yosemite_pretrained/latest_net_G.pth’ saved [45575747/45575747]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvtkjeEGOpDs",
        "colab_type": "text"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro6HaypdMrv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from io import BytesIO\n",
        "import subprocess\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NYndXtTM66F",
        "colab_type": "code",
        "outputId": "2b14ad3c-1e40-44b7-f5b5-7540148c7154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "a5bf3f43a0924c91980bed968bf07937",
            "baa9c0bdcbef4007a095456755e7553a",
            "208db3b7a80942b981795c1753b3458e",
            "329ba539bf244d4db19d2ac1f7c03bd7",
            "9092c0d95ecd4bdd879ad16725c850f6",
            "8057770337904687833f9c197cfaeb9b",
            "0a254352b8f64f68bb4d50474df44fa3",
            "06acd07746af4821be617a3a275dda16"
          ]
        }
      },
      "source": [
        "# В бейзлайне пример того, как мы можем обрабатывать две картинки, пришедшие от пользователя.\n",
        "model = StyleTransferModel()\n",
        "first_image_file = {}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5bf3f43a0924c91980bed968bf07937",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=574673361), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4OzbQjiA3pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def send_prediction_on_photo(update, context):\n",
        "    chat_id = update.message.chat_id\n",
        "    print(\"Got image from {}\".format(chat_id))\n",
        "    \n",
        "\n",
        "    image_file = update.message.photo[-1].get_file()\n",
        "\n",
        "    if chat_id in first_image_file:\n",
        "        update.message.reply_text('Принял вторую фотографию.Ожидайте 2 минуты')\n",
        "        # первая картинка, которая к нам пришла станет content image, а вторая style image\n",
        "        content_image_stream = BytesIO()\n",
        "        first_image_file[chat_id].download(out=content_image_stream)\n",
        "        del first_image_file[chat_id]\n",
        "\n",
        "        style_image_stream = BytesIO()\n",
        "        image_file.download(out=style_image_stream)\n",
        "        del image_file\n",
        "\n",
        "        output = model.transfer_style(content_image_stream, style_image_stream)\n",
        "\n",
        "        # теперь отправим назад фото\n",
        "        output_stream = BytesIO()\n",
        "        output.save(output_stream, format='PNG')\n",
        "        output_stream.seek(0)\n",
        "        update.message.reply_photo(photo=output_stream)\n",
        "        print(\"Sent Photo to user\")\n",
        "        update.message.reply_text('Пока! Если хочешь еще,напиши \"/start\".')\n",
        "        return ConversationHandler.END\n",
        "    else:\n",
        "        update.message.reply_text('Принял фото')\n",
        "        first_image_file[chat_id] = image_file\n",
        "        return PHOTO_1\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kun7-3Ov51oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def send_prediction_on_photo_CycleGAN(update, context):\n",
        "    os.chdir('/content/pytorch-CycleGAN-and-pix2pix/')\n",
        "    chat_id = update.message.chat_id\n",
        "    print(\"Got image from {}\".format(chat_id))\n",
        "    update.message.reply_text('Принял фото')\n",
        "\n",
        "    os.mkdir(str(chat_id))\n",
        "\n",
        "    image_file = update.message.photo[-1].get_file()\n",
        "    first_image_file[chat_id] = image_file\n",
        "    \n",
        "    image_path = str(chat_id)\n",
        "    first_image_file[chat_id].download(image_path+'/image.png')\n",
        "    del first_image_file[chat_id]\n",
        "\n",
        "    subprocess.call(['python','/content/pytorch-CycleGAN-and-pix2pix/test.py',\n",
        "    '--dataroot',image_path,'--name','winter2summer_yosemite_pretrained','--model','test',\n",
        "    '--no_dropout','--results','/content/'+image_path,'--load_size','720','--crop_size','720'])\n",
        "\n",
        "    shutil.rmtree('/content/pytorch-CycleGAN-and-pix2pix/' + image_path)\n",
        "    \n",
        "    output = Image.open('/content/'+image_path+ '/winter2summer_yosemite_pretrained/test_latest/images/image_fake.png')\n",
        "\n",
        "    output_stream = BytesIO()\n",
        "    output.save(output_stream, format='PNG')\n",
        "    output_stream.seek(0)\n",
        "    update.message.reply_photo(photo=output_stream)\n",
        "\n",
        "    shutil.rmtree('/content/' +image_path)\n",
        "    print(\"Sent Photo to user\")\n",
        "    update.message.reply_text('Пока! Если хочешь еще,напиши \"/start\".')\n",
        "    return ConversationHandler.END"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Y8h7TWOnVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def start(update, context):\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
        "    update.message.reply_text('Напиши мне \"/style\" ,и отправь две картинки .А я перенесу стиль второй картинки на первую')\n",
        "    update.message.reply_text('Напиши мне \"/gan\" ,и отправь одну зимнюю картинку.И я изменю ее на летнюю)')\n",
        "    update.message.reply_text('Остановить бота - \"/stop\"')\n",
        "    return NEURON_CHOISE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5f6ubpIrU3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cancel(update, context):\n",
        "    update.message.reply_text('Пока! Если хочешь еще,напиши \"/start\".')\n",
        "\n",
        "    return ConversationHandler.END"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM2WC-60RBAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEURON_CHOISE,PHOTO_1,PHOTO_2 = range(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urm0MZgjM_4D",
        "colab_type": "code",
        "outputId": "12cb8cff-b880-49aa-fb99-fac71ac76e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    token = \"\"\n",
        "    # Включим самый базовый логгинг, чтобы видеть сообщения об ошибках\n",
        "    logging.basicConfig(\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        level=logging.INFO)\n",
        "    \n",
        "    #, request_kwargs={'proxy_url': 'socks5h://163.172.152.192:1080'}\n",
        "    updater = Updater(token,use_context=True)\n",
        "    dp = updater.dispatcher\n",
        "\n",
        "    conv_handler = ConversationHandler(\n",
        "        entry_points=[CommandHandler('start', start)],\n",
        "\n",
        "        states={\n",
        "            NEURON_CHOISE: [CommandHandler('Style', lambda x,y: PHOTO_1),CommandHandler('Gan', lambda x,y: PHOTO_2)],\n",
        "            PHOTO_1: [MessageHandler(Filters.photo, send_prediction_on_photo),],\n",
        "            PHOTO_2: [MessageHandler(Filters.photo, send_prediction_on_photo_CycleGAN),],\n",
        "            \n",
        "        },\n",
        "\n",
        "        fallbacks=[CommandHandler('stop', cancel)]\n",
        "    )\n",
        "\n",
        "    start_button = InlineKeyboardButton('start')\n",
        "    InlineKeyboardMarkup(start_button)\n",
        "    \n",
        "    dp.add_handler(conv_handler)\n",
        "    run_async(conv_handler)\n",
        "    updater.start_polling()\n",
        "\n",
        "    updater.idle()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got image from 258862724\n",
            "Sent Photo to user\n",
            "Got image from 258862724\n",
            "Got image from 258862724\n",
            "cuda\n",
            "cuda\n",
            "Building the style transfer model..\n",
            "Optimizing..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "run [50]:\n",
            "Style Loss : 75756.375000 Content Loss: 11.073621\n",
            "\n",
            "run [100]:\n",
            "Style Loss : 56887.097656 Content Loss: 16.510307\n",
            "\n",
            "run [150]:\n",
            "Style Loss : 45065.167969 Content Loss: 22.019827\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 30939.123047 Content Loss: 23.791767\n",
            "\n",
            "run [250]:\n",
            "Style Loss : 1858546816.000000 Content Loss: 380.438324\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 39183.773438 Content Loss: 25.071609\n",
            "\n",
            "run [350]:\n",
            "Style Loss : 17056.933594 Content Loss: 24.910055\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 5569.303223 Content Loss: 24.194885\n",
            "\n",
            "run [450]:\n",
            "Style Loss : 516283616.000000 Content Loss: 221.784317\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 53711.492188 Content Loss: 23.487028\n",
            "\n",
            "Sent Photo to user\n",
            "Got image from 258862724\n",
            "Sent Photo to user\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-02-08 12:48:01,055 - telegram.ext.updater - INFO - Received signal 2 (SIGINT), stopping...\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
